{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author\n0  id26305  This process, however, afforded me no means of...    EAP\n1  id17569  It never once occurred to me that the fumbling...    HPL\n2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n3  id27763  How lovely is spring As we looked from Windsor...    MWS\n4  id12958  Finding nothing else, not even gold, the Super...    HPL\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writting syles differ in many ways but one such way is that some writers are wordier than others, and some use more or less punctuation than others. This seems especially relevant given that the writers being analysed use several different types of writing (Poe as a poet, Shelly as short stories, Lovecraft as a novelist). There is definitely cross over with these styles but I would argue that the stylistic aspects carry over throughout works so the following functions are designed to assess the writing sections for wordiness and punctuator usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text author  \\\n0  id26305  This process, however, afforded me no means of...    EAP   \n1  id17569  It never once occurred to me that the fumbling...    HPL   \n2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n\n   word count                                   punctuator_count  \\\n0          40  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n1          13  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n2          35  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n3          33  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n4          26  {'!': 0, '\"': 0, '#': 0, '$': 0, '%': 0, '&': ...   \n\n                                    punctuator_array  temp1  temp2  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...      7      3  \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...      1      1  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...      5      2  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...      4      2  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ...      4      3  \n  author  word count\n0    EAP   24.442405\n1    HPL   26.799645\n2    MWS   26.417273\n  author     temp1\n0    EAP  4.096329\n1    HPL  3.206921\n2    MWS  3.833719\n  author     temp2\n0    EAP  2.154177\n1    HPL  2.234960\n2    MWS  2.322799\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def get_word_counts(text):\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if i == \" \":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_punctuator_counts(text):\n",
    "    punct_dict = {}\n",
    "    for i in string.punctuation:\n",
    "        p_count = text.count(i)\n",
    "        punct_dict[i] = p_count\n",
    "    return punct_dict\n",
    "\n",
    "\n",
    "def get_punctuator_array(text):\n",
    "    punctuator_array_TF = []\n",
    "    for i in string.punctuation:\n",
    "        if i in text: \n",
    "            punctuator_array_TF.append(1)\n",
    "        else:\n",
    "            punctuator_array_TF.append(0)\n",
    "    return punctuator_array_TF\n",
    "\n",
    "df[\"word count\"] = df.text.apply(get_word_counts)\n",
    "df[\"punctuator_count\"] = df.text.apply(get_punctuator_counts)\n",
    "df[\"punctuator_array\"] = df.text.apply(get_punctuator_array)\n",
    "df[\"temp1\"] = df.punctuator_count.apply(lambda x: sum(x.values()))\n",
    "df[\"temp2\"] = df.punctuator_array.apply(lambda x: sum(x))\n",
    "\n",
    "print(df.head())\n",
    "print(df.groupby(\"author\", as_index=False)[\"word count\"].mean())\n",
    "print(df.groupby(\"author\", as_index=False)[\"temp1\"].mean())\n",
    "print(df.groupby(\"author\", as_index=False)[\"temp2\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data thus far does not look all that interesting but it does show that Poe tends to use slightly more punctuators and have fewer words (which makes sense for a poet), but also shows that HPL tends to use the least punctuation (somewhat surprising). It also reveals that wordcount/puntuation used might be a good metric to include. Additionally, another set of features that would be interesting to explore are metrics realated to readability. Here, this is looked at in brief with python's readability package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author    Kincaid\n0    EAP  11.345821\n1    HPL  11.949075\n2    MWS  11.641965\n  author        ARI\n0    EAP  12.290581\n1    HPL  13.636678\n2    MWS  13.181049\n  author  Coleman-Liau\n0    EAP      8.503033\n1    HPL      9.137809\n2    MWS      8.636868\n  author  FleschReadingEase\n0    EAP          59.146649\n1    HPL          59.136985\n2    MWS          60.534699\n  author  GunningFogIndex\n0    EAP        15.320904\n1    HPL        15.412072\n2    MWS        15.698662\n  author        LIX\n0    EAP  45.796967\n1    HPL  47.179099\n2    MWS  47.241839\n  author  SMOGIndex\n0    EAP  11.618737\n1    HPL  11.331048\n2    MWS  11.659334\n  author       RIX\n0    EAP  5.199747\n1    HPL  5.483230\n2    MWS  5.328756\n  author  characters_per_word\n0    EAP             4.451775\n1    HPL             4.480366\n2    MWS             4.431556\n  author  syll_per_word\n0    EAP       1.439720\n1    HPL       1.410766\n2    MWS       1.399661\n  author  words_per_sentence\n0    EAP           25.505443\n1    HPL           27.928305\n2    MWS           27.476837\n  author  characters\n0    EAP  113.687215\n1    HPL  125.836912\n2    MWS  121.408835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author  syllables\n0    EAP  37.118481\n1    HPL  39.758296\n2    MWS  38.444408\n  author      words\n0    EAP  25.505443\n1    HPL  27.928305\n2    MWS  27.476837\n  author  wordtypes\n0    EAP  21.744937\n1    HPL  24.472760\n2    MWS  23.404037\n  author  long_words\n0    EAP    5.199747\n1    HPL    5.483230\n2    MWS    5.328756\n  author  complex_words\n0    EAP       3.331139\n1    HPL       3.012422\n2    MWS       3.189940\n  author  tobeverb\n0    EAP  0.898734\n1    HPL  0.772671\n2    MWS  0.884844\n  author   auxverb\n0    EAP  0.306962\n1    HPL  0.288731\n2    MWS  0.446393\n  author  conjunction\n0    EAP     0.067975\n1    HPL     0.064596\n2    MWS     0.057743\n  author   pronoun\n0    EAP  0.310759\n1    HPL  0.289796\n2    MWS  0.386168\n  author  preposition\n0    EAP     0.155443\n1    HPL     0.140728\n2    MWS     0.109696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  author  nominalization\n0    EAP        0.470506\n1    HPL        0.259272\n2    MWS        0.434646\n  author  interrogative\n0    EAP       0.023924\n1    HPL       0.026264\n2    MWS       0.031602\n  author   article\n0    EAP  0.138228\n1    HPL  0.111269\n2    MWS  0.104070\n  author  subordination\n0    EAP       0.027342\n1    HPL       0.034073\n2    MWS       0.036069\n"
     ]
    }
   ],
   "source": [
    "import readability\n",
    "\n",
    "\n",
    "def get_readability_nums(textstr):\n",
    "    pop_list = ['sentences_per_paragraph', 'type_token_ratio', 'sentences', 'paragraphs']\n",
    "    res = dict(readability.getmeasures(text=textstr, merge=True))\n",
    "    for i in pop_list:\n",
    "        del res[i]\n",
    "    return res\n",
    "\n",
    "df[\"readability\"] = df.text.apply(get_readability_nums)\n",
    "keyslist = list(df.at[0, \"readability\"].keys())\n",
    "for i in keyslist:\n",
    "    df[i] = df.readability.apply(lambda x: x[i])\n",
    "    print(df.groupby(\"author\", as_index=False)[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features now need to be pipelined and labels seperated out. Important features will be pickedinitially using a decision tree to select for the features with the greatest information \n",
    "gain. Feature transformation will occur later on after there is a more solid grasp on the importance of individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word count         temp1         temp2       Kincaid           ARI  \\\ncount  19579.000000  19579.000000  19579.000000  19579.000000  19579.000000   \nmean      25.730477      3.759283      2.229481     11.610862     12.952885   \nstd       19.048353      3.009744      0.857384      8.095257      9.972436   \nmin        1.000000      1.000000      1.000000    -14.810000     -8.510000   \n25%       14.000000      2.000000      2.000000      6.727143      6.942500   \n50%       22.000000      3.000000      2.000000     10.680000     11.572174   \n75%       33.000000      5.000000      3.000000     15.327500     17.182973   \nmax      860.000000     71.000000      7.000000    336.810453    429.797282   \n\n       Coleman-Liau  FleschReadingEase  GunningFogIndex           LIX  \\\ncount  19579.000000       19579.000000     19579.000000  19579.000000   \nmean       8.727042          59.572356        15.463756     46.640785   \nstd        3.585138          28.608999         8.373508     21.600184   \nmin       -8.267856        -786.168502         0.800000      4.000000   \n25%        6.643838          42.594545        10.304762     34.666667   \n50%        8.887049          60.191765        14.430769     45.121212   \n75%       10.963043          76.820435        19.600000     56.948718   \nmax       34.083917         204.805000       349.092218    879.931475   \n\n          SMOGIndex      ...        complex_words      tobeverb       auxverb  \\\ncount  19579.000000      ...         19579.000000  19579.000000  19579.000000   \nmean      11.548470      ...             3.195822      0.858164      0.344757   \nstd        4.774881      ...             3.059622      0.986245      0.690141   \nmin        3.000000      ...             0.000000      0.000000      0.000000   \n25%        8.477226      ...             1.000000      0.000000      0.000000   \n50%       10.745967      ...             2.000000      1.000000      0.000000   \n75%       13.954451      ...             4.000000      1.000000      1.000000   \nmax       58.045436      ...           101.000000     24.000000     23.000000   \n\n        conjunction       pronoun   preposition  nominalization  \\\ncount  19579.000000  19579.000000  19579.000000    19579.000000   \nmean       0.063844      0.328004      0.137086        0.398641   \nstd        0.244481      0.469498      0.343947        0.734759   \nmin        0.000000      0.000000      0.000000        0.000000   \n25%        0.000000      0.000000      0.000000        0.000000   \n50%        0.000000      0.000000      0.000000        0.000000   \n75%        0.000000      1.000000      0.000000        1.000000   \nmax        1.000000      1.000000      1.000000       18.000000   \n\n       interrogative       article  subordination  \ncount   19579.000000  19579.000000   19579.000000  \nmean        0.026968      0.119924       0.031973  \nstd         0.161993      0.324881       0.175933  \nmin         0.000000      0.000000       0.000000  \n25%         0.000000      0.000000       0.000000  \n50%         0.000000      0.000000       0.000000  \n75%         0.000000      0.000000       0.000000  \nmax         1.000000      1.000000       1.000000  \n\n[8 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "text_features = df.drop(columns=[\"author\", \"punctuator_count\", \"punctuator_array\", \n",
    "                                 \"readability\", \"id\", \"text\"])\n",
    "text_labels = df[\"author\"].copy()\n",
    "print(text_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word count  temp1  temp2   Kincaid       ARI  Coleman-Liau  \\\n0          40      7    1.0  0.611773  0.843261      0.226461   \n1          13      1   -1.0 -0.949628 -0.666546     -0.662584   \n2          35      5    0.0  0.293011  0.579188      0.144228   \n3          33      4    0.0  0.383910  0.723531      0.849288   \n4          26      4    1.0  0.507386  0.548590      1.290869   \n\n   FleschReadingEase  GunningFogIndex       LIX  SMOGIndex      ...        \\\n0          -0.186881         0.841602  0.909657   1.035276      ...         \n1           1.221017        -0.642654 -0.434998  -0.414214      ...         \n2           0.127191         0.474832  0.463298   0.585786      ...         \n3          -0.140649         0.037191  0.820865  -0.414214      ...         \n4          -0.727376         0.246917  0.682707   0.585786      ...         \n\n   complex_words  tobeverb  auxverb  conjunction  pronoun  preposition  \\\n0       1.333333       0.0      1.0          0.0      1.0          0.0   \n1      -0.333333       0.0      1.0          0.0      1.0          0.0   \n2       0.666667       0.0      0.0          0.0      0.0          1.0   \n3      -0.333333       0.0      0.0          0.0      0.0          0.0   \n4       0.666667      -1.0      0.0          0.0      0.0          0.0   \n\n   nominalization  interrogative  article  subordination  \n0             0.0            0.0      0.0            0.0  \n1             0.0            0.0      0.0            0.0  \n2             1.0            0.0      0.0            0.0  \n3             0.0            1.0      0.0            0.0  \n4             1.0            0.0      0.0            0.0  \n\n[5 rows x 29 columns]\n          SMOGIndex           RIX  characters_per_word  syll_per_word\ncount  19579.000000  19579.000000         19579.000000   19579.000000\nmean       0.146516      0.264232             0.035948       0.033739\nstd        0.871770      0.884957             0.828544       0.804254\nmin       -1.414214     -0.800000            -3.624196      -4.787795\n25%       -0.414214     -0.400000            -0.481853      -0.483928\n50%        0.000000      0.000000             0.000000       0.000000\n75%        0.585786      0.600000             0.518147       0.516072\nmax        8.635662     31.800000             9.377606       7.104470\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "for column in text_features.columns[2:]:\n",
    "    shaped = np.array(text_features[column]).reshape(-1, 1)\n",
    "    scaler.fit(shaped)\n",
    "    text_features[column] = scaler.transform(shaped)\n",
    "print(text_features.head())\n",
    "test = text_features[['SMOGIndex', 'RIX', 'characters_per_word', 'syll_per_word']].copy()\n",
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear at this point exactly how well a robust scaler will work on some of the \n",
    "features such as word types, word complexity, verb count etc. So this will be revisited\n",
    "after training inital models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998212370397\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8962a8201aba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m                         cv=cv_sets)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(text_features, text_labels)\n",
    "guess = model1.predict(text_features)\n",
    "\n",
    "print(accuracy_score(text_labels, guess))\n",
    "\n",
    "parameters = {\"max_depth\": (1, 10, 100, 1000, None)}\n",
    "cv_sets = ShuffleSplit(n_splits= 10, test_size = 0.20, random_state = 0)\n",
    "scorer = make_scorer(score_func=accuracy_score)\n",
    "grid_obj = GridSearchCV(estimator=model1, scoring=scorer, param_grid=parameters, \n",
    "                        cv=cv_sets)\n",
    "grid_obj.fit(text_features, text_labels)\n",
    "\n",
    "print(grid_obj.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
